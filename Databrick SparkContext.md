
[('spark.databricks.preemption.enabled', 'true'), ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '1'), ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'), ('spark.driver.tempDirectory', '/local_disk0/tmp'), ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'), ('spark.databricks.managedCatalog.clientClassName',   'com.databricks.managedcatalog.ManagedCatalogClientImpl'), ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName',   'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'), ('spark.driver.extraJavaOptions',   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-exports=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',   '14.3.x-scala2.12'), ('spark.databricks.clusterUsageTags.clusterName', 'Check instance'), ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'), ('spark.sql.streaming.checkpointFileManagerClass',   'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'), ('spark.databricks.service.dbutils.repl.backend',   'com.databricks.dbconnect.ReplDBUtils'), ('spark.databricks.clusterUsageTags.managedResourceGroup',   'databricks-rg-duyld8_pyspark-iu2boxxk4pkjo'), ('spark.databricks.clusterUsageTags.clusterWorkers', '3'), ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'), ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'), ('spark.databricks.clusterSource', 'UI'), ('spark.hadoop.hive.server2.transport.mode', 'http'), ('spark.databricks.acl.dfAclsEnabled', 'false'), ('spark.driver.host', '10.139.64.4'), ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.driverPublicDns', '23.98.89.53'), ('spark.r.sql.derby.temp.dir', '/tmp/RtmpxC0PHP'), ('spark.repl.class.outputDir',   '/local_disk0/tmp/repl/spark-3939803911483251042-dbc75f26-6770-45a8-a6ea-5c8115a1f093'), ('spark.databricks.clusterUsageTags.hailEnabled', 'false'), ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'), ('spark.databricks.clusterUsageTags.containerType', 'LXC'), ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',   'shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider'), ('spark.eventLog.enabled', 'false'), ('spark.databricks.clusterUsageTags.sparkImageLabel',   'release__14.3.x-snapshot-scala2.12__databricks-universe__14.3.8__9ddb5b3__85fd02f__jenkins__76417bf__format-3'), ('spark.hadoop.fs.stage.impl.disable.cache', 'true'), ('spark.hadoop.hive.hmshandler.retry.interval', '2000'), ('spark.databricks.clusterUsageTags.clusterId', '0618-081331-epd3q2e1'), ('spark.executor.tempDirectory', '/local_disk0/tmp'), ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'), ('spark.hadoop.fs.fcfs-abfss.impl',   'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.hadoop.mapred.output.committer.class',   'com.databricks.backend.daemon.data.client.DirectOutputCommitter'), ('spark.hadoop.hive.server2.thrift.http.port', '10000'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3', '0'), ('spark.sql.allowMultipleContexts', 'false'), ('spark.databricks.eventLog.enabled', 'true'), ('spark.home', '/databricks/spark'), ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'), ('spark.task.reaper.enabled', 'true'), ('spark.storage.memoryFraction', '0.5'), ('spark.master', 'spark://10.139.64.4:7077'), ('spark.databricks.sql.configMapperClass',   'com.databricks.dbsql.config.SqlConfigMapperBridge'), ('spark.driver.maxResultSize', '4g'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'), ('spark.databricks.tahoe.logStore.r2.class',   'com.databricks.tahoe.store.R2LogStore'), ('spark.hadoop.fs.fcfs-s3.impl',   'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.delta.multiClusterWrites.enabled', 'true'), ('spark.databricks.clusterUsageTags.clusterLastActivityTime',   '1718857666187'), ('spark.worker.cleanup.enabled', 'false'), ('spark.sql.legacy.createHiveTableByDefault', 'false'), ('spark.databricks.driver.preferredMavenCentralMirrorUrl',   '[https://maven-central.storage-download.googleapis.com/maven2/')](https://maven-central.storage-download.googleapis.com/maven2/%27) "https://maven-central.storage-download.googleapis.com/maven2/%27)"), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File', '0'), ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'), ('spark.ui.port', '40001'), ('spark.databricks.wsfs.workspaceNotebookCwd', 'true'), ('spark.hadoop.fs.s3a.attempts.maximum', '10'), ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'), ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'), ('spark.hadoop.fs.azure.user.agent.prefix', ''), ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',   'false'), ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '3'), ('spark.hadoop.fs.fcfs-s3n.impl',   'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.hadoop.fs.abfs.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'), ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.clusterLogDestination', ''), ('spark.databricks.wsfsPublicPreview', 'true'), ('spark.cleaner.referenceTracking.blocking', 'false'), ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'false'), ('spark.databricks.clusterUsageTags.clusterState', 'Pending'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',   'false'), ('spark.databricks.tahoe.logStore.azure.class',   'com.databricks.tahoe.store.AzureLogStore'), ('spark.hadoop.fs.r2.impl.disable.cache', 'true'), ('spark.executor.extraJavaOptions',   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-exports=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib:/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:[http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory](http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory "http://www.w3.org/2001/xmlschema=com.sun.org.apache.xerces.internal.jaxp.validation.xmlschemafactory") -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'), ('spark.databricks.sparkContextId', '3939803911483251042'), ('spark.hadoop.fs.azure.skip.metrics', 'true'), ('spark.repl.class.uri', 'spark://10.139.64.4:36511/classes'), ('spark.hadoop.hive.hmshandler.retry.attempts', '10'), ('spark.hadoop.fs.wasb.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.scheduler.mode', 'FAIR'), ('spark.sql.sources.default', 'delta'), ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled',   'true'), ('spark.hadoop.fs.cpfs-s3n.impl',   'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.hadoop.fs.cpfs-adl.impl',   'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'), ('spark.hadoop.fs.cpfs-abfss.impl',   'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.databricks.passthrough.oauth.refresher.impl',   'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'), ('spark.databricks.universe.commandContextFactory.class',   'com.databricks.spark.util.UniverseCommandContextFactoryImpl'), ('spark.sql.hive.metastore.sharedPrefixes',   'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'), ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'), ('spark.task.reaper.killTimeout', '60s'), ('spark.hadoop.parquet.block.size.row.check.min', '10'), ('spark.hadoop.hive.server2.use.SSL', 'true'), ('spark.hadoop.spark.databricks.metrics.filesystem_metrics', 'true'), ('spark.hadoop.databricks.dbfs.client.version', 'v2'), ('spark.databricks.clusterUsageTags.driverNodeType', 'Standard_D4ds_v5'), ('spark.databricks.clusterUsageTags.driverInstanceId',   '0f2259b7d1ee4904b62642f482db7dff'), ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'), ('spark.hadoop.hive.server2.keystore.path',   '/databricks/keys/jetty-ssl-driver-keystore.jks'), ('spark.hadoop.fs.gs.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.databricks.credential.redactor',   'com.databricks.logging.secrets.CredentialRedactorProxyImpl'), ('spark.databricks.clusterUsageTags.clusterPinned', 'false'), ('spark.databricks.acl.provider',   'com.databricks.sql.acl.ReflectionBackedAclProvider'), ('spark.databricks.wsfs.workspacePrivatePreview', 'true'), ('spark.databricks.mlflow.autologging.enabled', 'true'), ('spark.extraListeners',   'com.databricks.backend.daemon.driver.DBCEventLoggingListener'), ('spark.databricks.clusterUsageTags.clusterMaxWorkers', '3'), ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',   'false'), ('spark.sql.parquet.cacheMetadata', 'true'), ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss', '0'), ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'), ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'), ('spark.hadoop.fs.abfss.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'), ('spark.databricks.tahoe.logStore.class',   'com.databricks.tahoe.store.DelegatingLogStore'), ('spark.hadoop.fs.s3.impl.disable.cache', 'true'), ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'), ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'), ('libraryDownload.sleepIntervalSeconds', '5'), ('spark.sql.hive.convertMetastoreParquet', 'true'), ('spark.databricks.service.dbutils.server.backend',   'com.databricks.dbconnect.SparkServerDBUtils'), ('spark.executor.id', 'driver'), ('spark.databricks.repl.enableClassFileCleanup', 'true'), ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '20'), ('spark.hadoop.fs.s3a.multipart.size', '10485760'), ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'), ('spark.akka.frameSize', '256'), ('spark.hadoop.fs.s3a.fast.upload', 'true'), ('spark.sql.streaming.stopTimeout', '15s'), ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'), ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting',   'false'), ('spark.hadoop.fs.s3a.retry.interval', '250ms'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'), ('spark.databricks.overrideDefaultCommitProtocol',   'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'), ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',   'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'), ('spark.databricks.clusterUsageTags.clusterMinWorkers', '3'), ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'), ('spark.hadoop.fs.adl.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('libraryDownload.timeoutSeconds', '180'), ('spark.hadoop.parquet.memory.pool.ratio', '0.5'), ('spark.hadoop.fs.local-file.impl',   'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'), ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',   'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'), ('spark.databricks.unityCatalog.legacy.enableCrossScopeCredCache', 'true'), ('spark.hadoop.fs.s3a.block.size', '67108864'), ('spark.databricks.tahoe.logStore.gcp.class',   'com.databricks.tahoe.store.GCPLogStore'), ('spark.serializer.objectStreamReset', '100'), ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'None'), ('spark.databricks.passthrough.enabled', 'false'), ('spark.sql.sources.commitProtocolClass',   'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs', '0'), ('spark.hadoop.fs.fcfs-s3a.impl',   'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''), ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',   'azure_disk_volume_type: PREMIUM_LRS\n'), ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'), ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'), ('spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders', 'true'), ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'), ('spark.hadoop.databricks.loki.fileSystemCache.enabled', 'true'), ('spark.shuffle.service.enabled', 'true'), ('spark.hadoop.fs.file.impl',   'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'), ('spark.plugins', 'org.apache.spark.sql.connect.SparkConnectPlugin'), ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'), ('spark.hadoop.fs.cpfs-s3.impl',   'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''), ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'), ('spark.rpc.message.maxSize', '256'), ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND_AZURE'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''), ('spark.hadoop.fs.fcfs-wasbs.impl',   'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.driverNfs.enabled', 'true'), ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',   'RDS_DIRECT'), ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'), ('spark.databricks.clusterUsageTags.azureSubscriptionId',   'a9e4cd6d-b268-42e2-bf68-ff1e23ca8f96'), ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'), ('spark.databricks.acl.enabled', 'false'), ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName',   'com.databricks.unity.TokenServiceApiTokenProvider'), ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',   'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'), ('spark.databricks.clusterUsageTags.enableElasticDisk', 'true'), ('spark.databricks.acl.scim.client',   'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'), ('spark.databricks.workspaceUrl',   'adb-1863672524590029.9.azuredatabricks.net'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'), ('spark.hadoop.fs.adl.impl.disable.cache', 'true'), ('spark.hadoop.parquet.block.size.row.check.max', '10'), ('spark.hadoop.fs.s3a.connection.maximum', '200'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'), ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'), ('spark.shuffle.reduceLocality.enabled', 'false'), ('spark.databricks.clusterUsageTags.clusterGeneration', '7'), ('spark.hadoop.spark.sql.sources.outputCommitterClass',   'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'), ('spark.hadoop.fs.fcfs-abfs.impl',   'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.hadoop.databricks.loki.fileStatusCache.enabled', 'true'), ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'), ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'), ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'), ('spark.sql.parquet.compression.codec', 'snappy'), ('spark.hadoop.fs.stage.impl',   'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'), ('spark.databricks.cloudProvider', 'Azure'), ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'southeastasia'), ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName',   'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'), ('spark.databricks.cloudfetch.hasRegionSupport', 'true'), ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',   'false'), ('spark.databricks.workerNodeTypeId', 'Standard_D4ds_v5'), ('spark.databricks.driverNodeTypeId', 'Standard_D4ds_v5'), ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'), ('spark.databricks.unityCatalog.enabled', 'false'), ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',   'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'), ('spark.databricks.clusterUsageTags.driverContainerPrivateIp', '10.139.64.4'), ('spark.sparklyr-backend.threads', '1'), ('spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice', '-1.0'), ('spark.hadoop.fs.fcfs-wasb.impl',   'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'), ('spark.databricks.passthrough.s3a.tokenProviderClassName',   'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'), ('spark.databricks.credential.scope.fs.onelake.tokenProviderClassName',   'com.databricks.backend.daemon.driver.credentials.CredentialScopeOneLakeTokenProvider'), ('spark.databricks.session.share', 'false'), ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'), ('spark.databricks.isShieldWorkspace', 'false'), ('spark.databricks.clusterUsageTags.region', 'southeastasia'), ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'), ('spark.databricks.telemetry.prometheus.samplingRate', '100'), ('spark.hadoop.fs.dbfs.impl',   'com.databricks.backend.daemon.data.client.DbfsHadoop3'), ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'), ('spark.hadoop.fs.gs.impl.disable.cache', 'true'), ('spark.databricks.privateLinkEnabled', 'false'), ('spark.delta.sharing.profile.provider.class',   'io.delta.sharing.DeltaSharingCredentialsProvider'), ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',   'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'), ('spark.hadoop.fs.wasbs.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.app.id', 'app-20240621050537-0000'), ('spark.databricks.clusterUsageTags.clusterScalingType', 'autoscaling'), ('spark.databricks.clusterUsageTags.sparkVersion', '14.3.x-scala2.12'), ('spark.databricks.automl.serviceEnabled', 'true'), ('spark.hadoop.parquet.page.size.check.estimate', 'false'), ('spark.databricks.clusterUsageTags.attribute_tag_service', ''), ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',   'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'), ('spark.databricks.metrics.filesystem_io_metrics', 'true'), ('spark.hadoop.spark.driverproxy.customHeadersToProperties',   'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims'), ('spark.databricks.credential.scope.fs.r2.tokenProviderClassName',   'com.databricks.backend.daemon.driver.credentials.CredentialScopeR2TokenProvider'), ('spark.databricks.clusterUsageTags.clusterNodeType', 'Standard_D4ds_v5'), ('spark.databricks.cloudfetch.requesterClassName',   'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'), ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'), ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '298643931807226'), ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'), ('spark.databricks.clusterUsageTags.userId', '298643931807226'), ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'), ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'), ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'), ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'), ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'), ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'), ('spark.speculation.quantile', '0.9'), ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'), ('spark.shuffle.manager', 'SORT'), ('spark.files.overwrite', 'true'), ('spark.databricks.credential.aws.secretKey.redactor',   'com.databricks.spark.util.AWSSecretKeyRedactorProxy'), ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'), ('spark.connect.extensions.command.classes',   'io.delta.connect.DeltaCommandPlugin'), ('spark.hadoop.fs.s3.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',   'false'), ('spark.r.numRBackendThreads', '1'), ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'), ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'), ('spark.hadoop.fs.azure.cache.invalidator.type',   'com.databricks.encryption.utils.CacheInvalidatorImpl'), ('spark.sql.hive.metastore.version', '0.13.0'), ('spark.shuffle.service.port', '4048'), ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'), ('spark.databricks.acl.client',   'com.databricks.spark.sql.acl.client.SparkSqlAclClient'), ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'), ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'), ('spark.databricks.clusterUsageTags.orgId', '1863672524590029'), ('spark.databricks.clusterUsageTags.runtimeEngine', 'STANDARD'), ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'), ('spark.databricks.credential.scope.fs.impl',   'com.databricks.sql.acl.fs.CredentialScopeFileSystem'), ('spark.databricks.enablePublicDbfsFuse', 'false'), ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'), ('spark.databricks.clusterUsageTags.driverContainerId',   '28e0b5c150524654a1778f16673e1a2a'), ('spark.databricks.passthrough.adls.tokenProviderClassName',   'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'), ('spark.app.name', 'Databricks Shell'), ('spark.driver.allowMultipleContexts', 'false'), ('spark.hadoop.fs.AbstractFileSystem.gs.impl',   'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'), ('spark.databricks.secret.sparkConf.keys.toRedact', ''), ('spark.rdd.compress', 'true'), ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',   'false'), ('spark.executor.memory', '8874m'), ('spark.hadoop.fs.s3a.retry.limit', '6'), ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''), ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'true'), ('spark.databricks.eventLog.dir', 'eventlogs'), ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'), ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName',   'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'), ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'), ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'NONE'), ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'), ('spark.speculation', 'false'), ('spark.databricks.clusterUsageTags.driverInstancePrivateIp', '10.139.0.11'), ('spark.hadoop.hive.server2.session.check.interval', '60000'), ('spark.sql.hive.convertCTAS', 'true'), ('spark.connect.extensions.relation.classes',   'io.delta.connect.DeltaRelationPlugin'), ('spark.hadoop.spark.sql.parquet.output.committer.class',   'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'), ('spark.hadoop.fs.s3a.max.total.tasks', '1000'), ('spark.databricks.tahoe.logStore.aws.class',   'com.databricks.tahoe.store.MultiClusterLogStore'), ('spark.hadoop.fs.s3a.fast.upload.default', 'true'), ('spark.hadoop.fs.r2.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.databricks.clusterUsageTags.workerEnvironmentId',   'workerenv-1863672524590029'), ('spark.hadoop.fs.mlflowdbfs.impl',   'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'), ('spark.databricks.eventLog.listenerClassName',   'com.databricks.backend.daemon.driver.DBCEventLoggingListener'), ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'), ('spark.speculation.multiplier', '3'), ('spark.storage.blockManagerTimeoutIntervalMs', '300000'), ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',   'workerenv-1863672524590029'), ('spark.sparkr.use.daemon', 'false'), ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'), ('spark.hadoop.fs.s3a.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'), ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'), ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'), ('spark.hadoop.fs.s3a.threads.max', '136'), ('spark.r.backendConnectionTimeout', '604800'), ('spark.ui.prometheus.enabled', 'true'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs', '0'), ('spark.hadoop.fs.s3n.impl',   'com.databricks.common.filesystem.LokiFileSystem'), ('spark.hadoop.hive.server2.idle.session.timeout', '900000'), ('spark.databricks.clusterUsageTags.effectiveSparkVersion',   '14.3.x-scala2.12'), ('spark.databricks.redactor',   'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'), ('spark.databricks.singleuser.fuse.scala.enabled', 'false'), ('spark.executor.extraClassPath',   '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'), ('spark.databricks.autotune.maintenance.client.classname',   'com.databricks.maintenanceautocompute.MACClientImpl'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes', '0'), ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '1863672524590029'), ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'), ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'), ('spark.app.startTime', '1718946332321'), ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace',   '0'), ('spark.databricks.clusterUsageTags.clusterAllTags',   '[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"duyld8@fpt.com"},{"key":"ClusterName","value":"Check instance"},{"key":"ClusterId","value":"0618-081331-epd3q2e1"},{"key":"created_by","value":"duyld8"},{"key":"DatabricksEnvironment","value":"workerenv-1863672524590029"}]'), ('spark.logConf', 'true'), ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'), ('spark.hadoop.hive.server2.enable.doAs', 'false'), ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'), ('spark.shuffle.memoryFraction', '0.2'), ('spark.driver.port', '36511'), ('spark.databricks.unityCatalog.volumes.fuse.server.enabled', 'true'), ('spark.hadoop.fs.dbfsartifacts.impl',   'com.databricks.backend.daemon.data.client.DBFSV1'), ('spark.hadoop.fs.cpfs-s3a.impl',   'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'), ('spark.hadoop.fs.s3a.connection.timeout', '50000'), ('spark.databricks.secret.envVar.keys.toRedact', ''), ('spark.databricks.clusterUsageTags.cloudProvider', 'Azure'), ('spark.files.useFetchCache', 'false')]