The most misconcept about Spark is that **Spark is in-memory technology**

Firsly, what kind of technology do we call in-memory? Following my research, in-memory technology refers to persist the data in RAM and effectively process it. What do we see in Spark? It has no option for in-memory data persistence, it has pluggable connectors for different persistent storage systems like HDFS, Cassandra, but it does not have native persistence code, neither for in-memory nor for on-disk storage. Everything Spark can do is caching the data, not **persistence**. Cached data can be easily dropped and recomputed later based on the other data available in the source persistent store available through connector.

Moreover, the heart of Spark, **shuffle**, writes data to disks. If you have a **groupBy()** transformation in SparkSQL query or you are just transforming RDD to PairRDD and calling on it some aggregation by key, you are forcing Spark to distrubute data amiong the partitions based on the hasj value of the key. he “shuffle” process consists of two phases, usually referred as “map” and “reduce”. “Map” just calculates hash values of your key (or other partitioning function if you set it manually) and outputs the data to **N** separate files on the local filesystem, where **N** is the number of partitions on the “reduce” side. “Reduce” side polls the “map” side for the data and merges it in new partitions. So if you have an RDD of **M** partitions and you transform it to pair RDD with **N** partitions, there would be **MxN** files created on the local filesystems in your cluster, holding all the data of the specific RDD. There are some optimizations available to reduce amount of files. Also there are some work under pre-sort them and then “merge” on “reduce” side, but this does not change the fact that each time you need to “shuffle” you data you are putting it to the HDDs.

Finally, Spark only a technology that allows you to utilize efficient in-memory LRU cache with possible on-disk eviction on memory